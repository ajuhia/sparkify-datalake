{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7c9df2",
   "metadata": {},
   "source": [
    "# Sparkify DataLake : using AWS EMR + S3\n",
    "- This file is to be run on a EMR cluster. (No need to explicity retrieve AWS credentials or create a spark session as we are directly executing code on AWS)\n",
    "- This notebook contains code for an ETL pipeline that reads input files from S3, processes them using pyspark and finally loads back the processed data to s3.\n",
    "- At the end, this notebook contains sample queries that can be executed by Sparkify Analytics team for exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b789f3d7",
   "metadata": {},
   "source": [
    "#### Import packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0f0fcc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0f4dc669fd4693b21eff79a403c1aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1679441267340_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-28-114.ec2.internal:20888/proxy/application_1679441267340_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-20-148.ec2.internal:8042/node/containerlogs/container_1679441267340_0001_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefdc952",
   "metadata": {},
   "source": [
    "####  Set Input and Output path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84e45683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4cde926e4749fdb96c02cc45f3418a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#input data path\n",
    "input_data = \"s3a://udacity-dend/\"\n",
    "#output data path\n",
    "output_data = \"s3://juhi-sparkify/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c6f830",
   "metadata": {},
   "source": [
    "#### Process input data files:\n",
    "\n",
    "- **Song.json**: processed by function **process_song_data()**\n",
    "- **log.json** : processed by function **process_log_data()**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05daaf97",
   "metadata": {},
   "source": [
    "#### process_song_data():\n",
    "  This function performs following tasks:<br><br>\n",
    "    1.  Reads song data from song dataset in s3, creates songs table and writes songs table to parquet files partitioned by year and artist.<br>\n",
    "    2.  Creates artists table using above song data and writes artists table to parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29746c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81292f6fd0e54a619916e344bfcc190d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_song_data(spark, input_data, output_data):\n",
    "    # get filepath to song data file\n",
    "    song_data = os.path.join(input_data, 'song_data/A/A/A/*.json')\n",
    "    \n",
    "    # read song data file\n",
    "    df = spark.read.json(song_data)\n",
    "\n",
    "    # extract columns to create songs table\n",
    "    songs_table = df.select(['song_id','title','artist_id','year','duration'])\n",
    "    \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    songs_table.write.mode(\"overwrite\").parquet(os.path.join(output_data, 'songs'), partitionBy=['year', 'artist_id'])\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    artists_table = df.select(['artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude'])\n",
    "    \n",
    "    # write artists table to parquet files\n",
    "    artists_table.write.mode(\"overwrite\").parquet(os.path.join(output_data, 'artists'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8933ed8d",
   "metadata": {},
   "source": [
    "#### process_log_data():\n",
    " This function performs following tasks:<br><br>\n",
    " Reads log data from log dataset in s3, based on action: 'NextSong',  creates users,time, and songplays table and  writes these tables to paraquet files. Songsplays table is partitioned partitioned by year and artist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "521f9cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "451344781fb942db95e80c2aa2dd76d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_log_data(spark, input_data, output_data):\n",
    "    # get filepath to log data file\n",
    "    log_data = os.path.join(input_data, 'log_data/*/*/*.json')\n",
    "\n",
    "    # read log data file\n",
    "    df = spark.read.json(log_data)\n",
    "    \n",
    "    # filter by actions for song plays\n",
    "    df = df.where(df.page == 'NextSong')\n",
    "\n",
    "    # extract columns for users table    \n",
    "    users_table = df.select(['userId', 'firstName', 'lastName', 'gender', 'level'])\n",
    "    users_table = users_table.drop_duplicates(subset=['userId'])\n",
    "    \n",
    "    # write users table to parquet files\n",
    "    users_table.write.mode(\"overwrite\").parquet(os.path.join(output_data, 'users'))\n",
    "\n",
    "    # create timestamp column from original timestamp column\n",
    "    get_timestamp = udf(lambda x: datetime.fromtimestamp(x/1000).strftime('%Y-%m-%d'))\n",
    "    df = df.withColumn('timestamp', get_timestamp('ts'))\n",
    "    \n",
    "    # create datetime column from original timestamp column\n",
    "    get_datetime = udf(lambda x: str(datetime.fromtimestamp(int(x) / 1000)))\n",
    "    df = df.withColumn('datetime', get_datetime(df.ts))\n",
    "    \n",
    "    \n",
    "    # extract columns to create time table\n",
    "    time_table = df.select(\n",
    "                 F.col(\"timestamp\").alias(\"start_time\"),\n",
    "                 F.hour(\"timestamp\").alias('hour'),\n",
    "                 F.dayofmonth(\"timestamp\").alias('day'),\n",
    "                 F.weekofyear(\"timestamp\").alias('week'),\n",
    "                 F.month(\"timestamp\").alias('month'), \n",
    "                 F.year(\"timestamp\").alias('year'), \n",
    "                 F.date_format(F.col(\"timestamp\"), \"E\").alias(\"weekday\")\n",
    "            )\n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    time_table.write.mode(\"overwrite\").parquet(os.path.join(output_data, 'time'), partitionBy=['year', 'month'])\n",
    "    \n",
    "    # read in song data to use for songplays table\n",
    "    song_df = spark.read.json(input_data + 'song_data/A/A/A/*.json')\n",
    "\n",
    "    # extract columns from joined song and log datasets to create songplays table \n",
    "    df = df.alias('log_df')\n",
    "    song_df    = song_df.alias('song_df')\n",
    "    new_df  = df.join(song_df, col('log_df.artist') == col(\n",
    "        'song_df.artist_name'), 'inner')\n",
    "    \n",
    "    songplays_table = new_df.select(\n",
    "            col('log_df.datetime').alias('start_time'),\n",
    "            col('log_df.userId').alias('user_id'),\n",
    "            col('log_df.level').alias('level'),\n",
    "            col('song_df.song_id').alias('song_id'),\n",
    "            col('song_df.artist_id').alias('artist_id'),\n",
    "            col('log_df.sessionId').alias('session_id'),\n",
    "            col('log_df.location').alias('location'), \n",
    "            col('log_df.userAgent').alias('user_agent'),\n",
    "            year('log_df.datetime').alias('year'),\n",
    "            month('log_df.datetime').alias('month')) \\\n",
    "            .withColumn('songplay_id', F.monotonically_increasing_id())\n",
    "\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    songplays_table.write.mode(\"overwrite\").parquet(os.path.join(output_data, 'songplays'), partitionBy=['year', 'month'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2057a47",
   "metadata": {},
   "source": [
    "#### Execute the above defined functions to load, process and store data on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a571f841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b941c7f9bd184013a3e284ab1f63d97d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 36084)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 351, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 364, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 724, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 266, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 254, in authenticate_and_accum_updates\n",
      "    received_token = self.rfile.read(len(auth_token))\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "----------------------------------------"
     ]
    }
   ],
   "source": [
    "#Process song files\n",
    "process_song_data(spark, input_data, output_data)  \n",
    "#Process log files\n",
    "process_log_data(spark, input_data, output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f05ade",
   "metadata": {},
   "source": [
    "#### All the steps of ETL pipeline have been exexuted : Processed data has been uploaded to \"s3://juhi-sparkify/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95807a1f",
   "metadata": {},
   "source": [
    "# Sample Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9d7d7b",
   "metadata": {},
   "source": [
    "#### 1.  Find the total number of paid and free users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df6bca09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b9c88f54717418ab0e2c634ac5cc33b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+\n",
      "|subscription_level|total_count|\n",
      "+------------------+-----------+\n",
      "|              free|         75|\n",
      "|              paid|         21|\n",
      "+------------------+-----------+"
     ]
    }
   ],
   "source": [
    "users_df = spark.read.parquet(\"s3a://juhi-sparkify/users\")\n",
    "users_df.createOrReplaceTempView(\"users\")\n",
    "spark.sql(\"\"\"\n",
    "    select\n",
    "        level as subscription_level,\n",
    "        count(userid) as total_count\n",
    "    from \n",
    "        users\n",
    "    group by 1\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41954ee7",
   "metadata": {},
   "source": [
    "#### 2. Total number of songs played year wise in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5abbbb67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f46003e36d684e958095a98dc4af4ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+\n",
      "|year|songplay_count|\n",
      "+----+--------------+\n",
      "|2018|            10|\n",
      "+----+--------------+"
     ]
    }
   ],
   "source": [
    "songplays_df = spark.read.parquet(\"s3a://juhi-sparkify/songplays\")\n",
    "songplays_df.createOrReplaceTempView(\"songplays\")\n",
    "spark.sql(\"\"\"\n",
    "    select\n",
    "        year,\n",
    "        count(*) as songplay_count\n",
    "    from \n",
    "        songplays\n",
    "    group by 1\n",
    "    order by 2 desc\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
